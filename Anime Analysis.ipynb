{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Analysis by Ben Osborn and OsbornAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project consists of the scraping and creation of a dataset containing information about all anime's listed on MyAnimeList. This data is analysed, and a model is created to predict the anime's rating based on the pages features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import DictWriter\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parses through the labels from the soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name to clean label\n",
    "def parseLabel(element):\n",
    "    string = element.text\n",
    "    \n",
    "    split_colens = string.split(':')\n",
    "    removed_label = split_colens[1:]\n",
    "    \n",
    "    for i, label in enumerate(removed_label):\n",
    "        removed_label[i] = label.replace('\\n', '').strip()\n",
    "    \n",
    "    joined = \" \".join(removed_label)\n",
    "    \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parses through the list soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseList(element): # Have to check that a tags exist for ever single page and developer\n",
    "    ret_list = [a.text for a in element.find_all('a')]\n",
    "    \n",
    "    return \", \".join(ret_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the field names globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = ['name_english', 'name_japanese', 'show_type', 'episodes', 'status', 'aired', 'broadcast_time', 'producers', \n",
    "               'licensors', 'studios', 'source', 'genres', 'episode_length', 'rating', 'score_and_scorers', \n",
    "               'members', 'favorites', 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapes the page from the show and returns a row of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRow(url):\n",
    "    global field_names\n",
    "    ret_dict = {field_name: '' for field_name in field_names}\n",
    "\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "    side_panel = soup.find('td', class_='borderClass')\n",
    "    side_panel_subdiv = side_panel.find('div')\n",
    "    side_panel_divs = side_panel_subdiv.find_all('div')\n",
    "\n",
    "    try:\n",
    "        ret_dict['description'] = soup.find('p', itemprop='description').text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error '{e}' for description at '{url}'.\")\n",
    "\n",
    "    for panel in side_panel_divs:\n",
    "        try:\n",
    "            split = str(panel.text.split(':')[0].strip())\n",
    "\n",
    "            if split == \"English\":\n",
    "                ret_dict['name_english'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Japanese\":\n",
    "                ret_dict['name_japanese'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Type\":\n",
    "                ret_dict['show_type'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Episodes\":\n",
    "                ret_dict['episodes'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Status\":\n",
    "                ret_dict['status'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Aired\":\n",
    "                ret_dict['aired'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Broadcast\":\n",
    "                ret_dict['broadcast_time'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Producers\":\n",
    "                ret_dict['producers'] = parseList(panel)\n",
    "\n",
    "            if split == \"Licensors\":\n",
    "                ret_dict['licensors'] = parseList(panel)\n",
    "\n",
    "            if split == \"Studios\":\n",
    "                ret_dict['studios'] = parseList(panel)\n",
    "\n",
    "            if split == \"Source\":\n",
    "                ret_dict['source'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Genres\":\n",
    "                ret_dict['genres'] = parseList(panel)\n",
    "\n",
    "            if split == \"Duration\":\n",
    "                ret_dict['episode_length'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Rating\":\n",
    "                ret_dict['rating'] = parseLabel(panel).split(' ')[0]\n",
    "\n",
    "            if split == \"Score\":\n",
    "                ret_dict['score_and_scorers'] = \", \".join([part.text for part in panel.find_all('span')][1:])\n",
    "\n",
    "            if split == \"Members\":\n",
    "                ret_dict['members'] = \"\".join(parseLabel(panel).split(','))\n",
    "\n",
    "            if split == \"Favorites\":\n",
    "                ret_dict['favorites'] = \"\".join(parseLabel(panel).split(','))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered an error '{e}' at '{url}'.\")\n",
    "            \n",
    "    return ret_dict\n",
    "    \n",
    "# createRow(\"https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through the amount of pages specified then scrape the information for each show, then store them to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce sharding of CSV to this\n",
    "\n",
    "def genDataset(end_page, csv_filename, start_page=0): # Where resume is the page of which it left off from\n",
    "    global field_names\n",
    "    link = ['Unknown']\n",
    "            \n",
    "    for i in range(start_page, end_page):\n",
    "        \n",
    "        print(f\"Scraping page {i}...\")\n",
    "\n",
    "        csv_path = os.path.join(os.getcwd(), 'csv', f'{csv_filename}-{i}.csv')\n",
    "\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            writer = DictWriter(csvfile, fieldnames=field_names)\n",
    "            \n",
    "            writer.writeheader()\n",
    "\n",
    "            url_page = f\"https://myanimelist.net/topanime.php?limit={i*50}\"\n",
    "            req_list = requests.get(url_page)\n",
    "            soup_list = BeautifulSoup(req_list.content, 'html.parser')\n",
    "            shows = soup_list.find_all('tr', class_='ranking-list')\n",
    "\n",
    "            for show in shows:\n",
    "                try:\n",
    "                    link[0] = show.find('a').get('href')\n",
    "                    data_row = createRow(link[0])\n",
    "                    writer.writerow(data_row)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Encountered error '{e}' at '{link[0]}'.\")\n",
    "    \n",
    "    print(\"Dataset creation complete!\")\n",
    "    \n",
    "# Testing on a small dataset to begin with\n",
    "# genDataset(5, 'mal-data-11-13-2020', start_page=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate CSV files and put them into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'csv')\n",
    "\n",
    "dfs = []\n",
    "for csv in os.listdir(data_path):\n",
    "    dfs.append(pd.read_csv(os.path.join(data_path, csv), index_col=0))\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type conversion and data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['episodes'] != 'Unknown']\n",
    "df['episodes'] = df['episodes'].astype(int)\n",
    "\n",
    "df['favorites'] = df['favorites'].astype(int)\n",
    "\n",
    "df['members'] = df['members'].astype(int)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseScoreAndScorer(score_and_scorers_raw, position):\n",
    "    try:\n",
    "        numerical_version = float(score_and_scorers_raw.split(', ')[position].strip())\n",
    "        return numerical_version\n",
    "    except:\n",
    "        return pd.NaT\n",
    "    \n",
    "df['score'] = df['score_and_scorers'].apply(lambda s: parseScoreAndScorer(s, 0)).astype(float)\n",
    "df['scorer'] = df['score_and_scorers'].apply(lambda s: parseScoreAndScorer(s, 1)).astype(int)\n",
    "df = df.dropna()\n",
    "df = df.drop('score_and_scorers', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcastParse(broadcast_raw): # This parses through the broadcast time\n",
    "    broadcast_split = [bc_time.strip() for bc_time in broadcast_raw.split(' at ')]\n",
    "    if (len(broadcast_split) == 1) or ('Unknown' in broadcast_split):\n",
    "        return pd.NaT\n",
    "    broadcast_split[0] = broadcast_split[0][:-1]\n",
    "    broadcast_split[1] = broadcast_split[1][:5]\n",
    "    broadcast_string = \" \".join(broadcast_split)\n",
    "    \n",
    "    return broadcast_string\n",
    "\n",
    "df['broadcast_time'] = df['broadcast_time'].apply(broadcastParse)\n",
    "df = df.dropna()\n",
    "df['broadcast_time'] = pd.to_datetime(df['broadcast_time'], format='%A %H %M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one needs to parse the minutes/hours for the episode length\n",
    "def timeParse(time_raw):\n",
    "    time_split = time_raw.split(' ')\n",
    "    if time_split[1] == 'min.':\n",
    "        return int(time_split[0])\n",
    "    elif time_split[1] == 'hr.':\n",
    "        return int(time_split[0]) * 60 + int(time_split[2]) # This will cause an error if it has different formatting\n",
    "    \n",
    "df['episode_length'] = df['episode_length'].apply(timeParse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAired(aired_raw, position):\n",
    "    aired_split = aired_raw.split(' to ')\n",
    "    try:\n",
    "        aired_parsed = \" \".join(aired_split[position].strip().split(\", \"))\n",
    "        aired_formatted = str(time.strftime('%d %m %Y', time.strptime(aired_parsed, '%b %d %Y')))\n",
    "        return aired_formatted\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "df['aired_start'] = pd.to_datetime(df['aired'].apply(lambda s: parseAired(s, 0)), format='%d %m %Y')\n",
    "df['aired_end'] = pd.to_datetime(df['aired'].apply(lambda s: parseAired(s, 1)), format='%d %m %Y')\n",
    "df = df.dropna()\n",
    "df = df.drop('aired', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name_japanese                                                黒子のバスケ\n",
       "show_type                                                        TV\n",
       "episodes                                                        201\n",
       "status                                              Finished Airing\n",
       "broadcast_time                                  1900-01-01 23:45:00\n",
       "producers                                                  add some\n",
       "licensors                                                  add some\n",
       "studios                                                    ufotable\n",
       "source                                                    Web manga\n",
       "genres                                             Thriller, Sci-Fi\n",
       "episode_length                                                   50\n",
       "rating                                                           R+\n",
       "members                                                     2329007\n",
       "favorites                                                    167719\n",
       "description       Young Thorfinn grew up listening to the storie...\n",
       "score                                                          9.22\n",
       "scorer                                                      1589538\n",
       "aired_start                                     2020-07-10 00:00:00\n",
       "aired_end                                       2020-09-30 00:00:00\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
