{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Analysis by Ben Osborn and OsbornAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project consists of the scraping and creation of a dataset containing information about all anime's listed on MyAnimeList. This data is analysed, and a model is created to predict the anime's rating based on the pages features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parses through the labels from the soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name to clean label\n",
    "def parseLabel(element):\n",
    "    string = element.text\n",
    "    \n",
    "    split_colens = string.split(':')\n",
    "    removed_label = split_colens[1:]\n",
    "    \n",
    "    for i, label in enumerate(removed_label):\n",
    "        removed_label[i] = label.replace('\\n', '').strip()\n",
    "    \n",
    "    joined = \" \".join(removed_label)\n",
    "    \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parses through the list soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseList(element): # Have to check that a tags exist for ever single page and developer\n",
    "    ret_list = [a.text for a in element.find_all('a')]\n",
    "    \n",
    "    return \", \".join(ret_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapes the page from the show and returns a row of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRow(url, field_names):\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "    description = soup.find('p', itemprop='description').text.replace('\\n', '') # --------- Check this one\n",
    "\n",
    "    side_panel = soup.find('td', class_='borderClass')\n",
    "    side_panel_subdiv = side_panel.find('div')\n",
    "    side_panel_divs = side_panel_subdiv.find_all('div')\n",
    "    \n",
    "    name = parseLabel(side_panel_divs[7]) # Good\n",
    "    show_type = parseLabel(side_panel_divs[10]) # Good\n",
    "    episodes = parseLabel(side_panel_divs[11]) # Good\n",
    "    status = parseLabel(side_panel_divs[12]) # Good\n",
    "    \n",
    "    aired_raw = parseLabel(side_panel_divs[13]) \n",
    "    aired = [time.strftime('%d-%m-%Y', time.strptime(date.strip().replace(',', ''), '%b %d %Y')) for date in aired_raw.split(\"to\")]\n",
    "    aired_start = aired[0] # Good\n",
    "    aired_end = aired[1] # Good\n",
    "    \n",
    "    broadcast_time_raw = parseLabel(side_panel_divs[15])\n",
    "    broadcast_time_split = [element.strip() for element in broadcast_time_raw.split('at')]\n",
    "    broadcast_time_split[0] = broadcast_time_split[0][:-1]\n",
    "    broadcast_time_split[1] = broadcast_time_split[1][:5]\n",
    "    broadcast_time_joined = \" \".join(broadcast_time_split)\n",
    "    broadcast_time = time.strftime('%A %H:%M', time.strptime(broadcast_time_joined, '%A %H %M')) # Good (Possibly unnecessary)\n",
    "    \n",
    "    producers = parseList(side_panel_divs[16]) # Good\n",
    "    licensors = parseList(side_panel_divs[17]) # Good\n",
    "    studios = parseList(side_panel_divs[18]) # Good\n",
    "    source = parseLabel(side_panel_divs[19]) # Good\n",
    "    genres = parseList(side_panel_divs[20]) # Good\n",
    "    \n",
    "    episode_length_raw = parseLabel(side_panel_divs[21])\n",
    "    episode_length = episode_length_raw.split(' ')[0] # Good\n",
    "    \n",
    "    rating_raw = parseLabel(side_panel_divs[22])\n",
    "    rating = rating_raw.split(' ')[0] # Good\n",
    "    \n",
    "    score_and_scorers = \", \".join([part.text for part in side_panel_divs[23].find_all('span')][1:]) # Good\n",
    "        \n",
    "    members = \"\".join(parseLabel(side_panel_divs[28]).split(',')) # Good\n",
    "    favourites = \"\".join(parseLabel(side_panel_divs[29]).split(',')) # Good\n",
    "    \n",
    "    ret_list = name, show_type, episodes, status, aired_start, aired_end, broadcast_time, producers, licensors, studios, \\\n",
    "            source, genres, episode_length, rating, score_and_scorers, members, favourites, description\n",
    "    \n",
    "    assert(len(ret_list) == len(field_names)), f\"Length of fieldname's should match be of size {len(ret_list)}\"\n",
    "    \n",
    "    ret_dict = {key: value for key, value in zip(field_names, ret_list)}\n",
    "    \n",
    "    return ret_dict\n",
    "    \n",
    "# createRow(\"https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\", ['name', 'type', 'episodes', 'status', 'aired_start', 'aired_end', \n",
    "#                       'broadcast_time', 'producers', 'licensors', 'studios', 'source',\n",
    "#                       'genres', 'episode_length', 'rating', 'score_and_scorers', 'members',\n",
    "#                       'favourites', 'description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through the amount of pages specified then scrape the information for each show, then store them to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithread this to make it faster\n",
    "\n",
    "def genDataset(pages_to_scrape, csv_filename, start_page=0): # Where resume is the page of which it left off from\n",
    "    fieldnames = ['name', 'type', 'episodes', 'status', 'aired_start', 'aired_end', \n",
    "                      'broadcast_time', 'producers', 'licensors', 'studios', 'source',\n",
    "                      'genres', 'episode_length', 'rating', 'score_and_scorers', 'members',\n",
    "                      'favourites', 'description']\n",
    "    \n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        for i in range(start_page, pages_to_scrape):\n",
    "            url_page = f\"https://myanimelist.net/topanime.php?limit={i*50}\"\n",
    "            req_list = requests.get(url_page)\n",
    "            soup_list = BeautifulSoup(req_list.content, 'html.parser')\n",
    "            shows = soup_list.find_all('tr', class_='ranking-list')\n",
    "\n",
    "            for show in shows:\n",
    "                link = show.find('a').get('href')\n",
    "                data_row = createRow(link, fieldnames)\n",
    "                print(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Fullmetal Alchemist Brotherhood', 'type': 'TV', 'episodes': '64', 'status': 'Finished Airing', 'aired_start': '05-04-2009', 'aired_end': '04-07-2010', 'broadcast_time': 'Sunday 17:00', 'producers': 'Aniplex, Square Enix, Mainichi Broadcasting System, Studio Moriken', 'licensors': 'Funimation, Aniplex of America', 'studios': 'Bones', 'source': 'Manga', 'genres': 'Action, Military, Adventure, Comedy, Drama, Magic, Fantasy, Shounen', 'episode_length': '24', 'rating': 'R', 'score_and_scorers': '9.22, 1226238', 'members': '2022089', 'favourites': '167577', 'description': '\"In order for something to be obtained, something of equal value must be lost.\"\\rAlchemy is bound by this Law of Equivalent Exchange—something the young brothers Edward and Alphonse Elric only realize after attempting human transmutation: the one forbidden act of alchemy. They pay a terrible price for their transgression—Edward loses his left leg, Alphonse his physical body. It is only by the desperate sacrifice of Edward\\'s right arm that he is able to affix Alphonse\\'s soul to a suit of armor. Devastated and alone, it is the hope that they would both eventually return to their original bodies that gives Edward the inspiration to obtain metal limbs called \"automail\" and become a state alchemist, the Fullmetal Alchemist.\\rThree years of searching later, the brothers seek the Philosopher\\'s Stone, a mythical relic that allows an alchemist to overcome the Law of Equivalent Exchange. Even with military allies Colonel Roy Mustang, Lieutenant Riza Hawkeye, and Lieutenant Colonel Maes Hughes on their side, the brothers find themselves caught up in a nationwide conspiracy that leads them not only to the true nature of the elusive Philosopher\\'s Stone, but their country\\'s murky history as well. In between finding a serial killer and racing against time, Edward and Alphonse must ask themselves if what they are doing will make them human again... or take away their humanity.\\r[Written by MAL Rewrite]'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data 'Wednesdays at 02 05 (JST)' does not match format '%b %d %Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-378079de49b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mal-data-12-11-2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-b4fa22e93068>\u001b[0m in \u001b[0;36mgenDataset\u001b[0;34m(pages_to_scrape, csv_filename, start_page)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mshow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mdata_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-66795af60d99>\u001b[0m in \u001b[0;36mcreateRow\u001b[0;34m(url, field_names)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0maired_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mside_panel_divs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0maired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d-%m-%Y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%b %d %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maired_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0maired_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maired\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0maired_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maired\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-66795af60d99>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0maired_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mside_panel_divs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0maired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d-%m-%Y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%b %d %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maired_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0maired_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maired\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0maired_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maired\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime_time\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \"\"\"Return a time struct based on the input string and the\n\u001b[1;32m    561\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STRUCT_TM_ITEMS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[1;32m    350\u001b[0m                          (data_string, format))\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data 'Wednesdays at 02 05 (JST)' does not match format '%b %d %Y'"
     ]
    }
   ],
   "source": [
    "genDataset(10, 'mal-data-12-11-2020.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
