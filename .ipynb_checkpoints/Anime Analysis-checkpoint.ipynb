{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Analysis by Ben Osborn and OsbornAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project consists of the scraping and creation of a dataset containing information about all anime's listed on MyAnimeList. This data is analysed, and a model is created to predict the anime's rating based on the pages features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parses through the labels and the lists from the soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parseList(element): # Have to check that a tags exist for ever single page and developer\n",
    "    ret_list = [a.text for a in element.find_all('a')]\n",
    "    \n",
    "    return \", \".join(ret_list)\n",
    "\n",
    "def parseLabel(element):\n",
    "    string = element.text\n",
    "    \n",
    "    split_colens = string.split(':')\n",
    "    removed_label = split_colens[1:]\n",
    "    \n",
    "    for i, label in enumerate(removed_label):\n",
    "        removed_label[i] = label.replace('\\n', '').strip()\n",
    "    \n",
    "    joined = \" \".join(removed_label)\n",
    "    \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are the names of the raw initial columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = ['name_english', 'name_japanese', 'show_type', 'episodes', 'status', 'aired', 'broadcast_time', 'producers', \n",
    "               'licensors', 'studios', 'source', 'genres', 'episode_length', 'rating', 'score_and_scorers', \n",
    "               'members', 'favorites', 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapes the page from the show and returns a row of data for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def createRow(url):\n",
    "    ret_dict = {field_name: '' for field_name in field_names}\n",
    "\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "    side_panel = soup.find('td', class_='borderClass')\n",
    "    side_panel_subdiv = side_panel.find('div')\n",
    "    side_panel_divs = side_panel_subdiv.find_all('div')\n",
    "\n",
    "    try:\n",
    "        ret_dict['description'] = soup.find('p', itemprop='description').text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error '{e}' for description at '{url}'.\")\n",
    "\n",
    "    for panel in side_panel_divs:\n",
    "        try:\n",
    "            split = str(panel.text.split(':')[0].strip())\n",
    "\n",
    "            if split == \"English\":\n",
    "                ret_dict['name_english'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Japanese\":\n",
    "                ret_dict['name_japanese'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Type\":\n",
    "                ret_dict['show_type'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Episodes\":\n",
    "                ret_dict['episodes'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Status\":\n",
    "                ret_dict['status'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Aired\":\n",
    "                ret_dict['aired'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Broadcast\":\n",
    "                ret_dict['broadcast_time'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Producers\":\n",
    "                ret_dict['producers'] = parseList(panel)\n",
    "\n",
    "            if split == \"Licensors\":\n",
    "                ret_dict['licensors'] = parseList(panel)\n",
    "\n",
    "            if split == \"Studios\":\n",
    "                ret_dict['studios'] = parseList(panel)\n",
    "\n",
    "            if split == \"Source\":\n",
    "                ret_dict['source'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Genres\":\n",
    "                ret_dict['genres'] = parseList(panel)\n",
    "\n",
    "            if split == \"Duration\":\n",
    "                ret_dict['episode_length'] = parseLabel(panel)\n",
    "\n",
    "            if split == \"Rating\":\n",
    "                ret_dict['rating'] = parseLabel(panel).split(' ')[0]\n",
    "\n",
    "            if split == \"Score\":\n",
    "                ret_dict['score_and_scorers'] = \", \".join([part.text for part in panel.find_all('span')][1:])\n",
    "\n",
    "            if split == \"Members\":\n",
    "                ret_dict['members'] = \"\".join(parseLabel(panel).split(','))\n",
    "\n",
    "            if split == \"Favorites\":\n",
    "                ret_dict['favorites'] = \"\".join(parseLabel(panel).split(','))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Encountered an error '{e}' at '{url}'.\")\n",
    "            \n",
    "    return ret_dict\n",
    "    \n",
    "# createRow(\"https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through the amount of pages specified then scrape the information for each show, then store them to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0208f7cacd4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dataset creation complete!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mgenDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mal-data-20-11-2020'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_page\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-0208f7cacd4a>\u001b[0m in \u001b[0;36mgenDataset\u001b[1;34m(end_page, csv_filename, start_page)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Scraping page {i}...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mcsv_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'{csv_filename}-{i}.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Introduce sharding of CSV to this\n",
    "import os\n",
    "from csv import DictWriter\n",
    "import time\n",
    "\n",
    "# Change the file saving location for this\n",
    "def genDataset(end_page, csv_path, csv_filename, start_page=0): # Where resume is the page of which it left off from\n",
    "    link = 'Unknown'\n",
    "            \n",
    "    for i in range(start_page, end_page):\n",
    "        \n",
    "        print(f\"Scraping page {i}...\")\n",
    "\n",
    "        csv_path = os.path.join(csv_path, f'{csv_filename}-{i}.csv')\n",
    "\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            writer = DictWriter(csvfile, fieldnames=field_names)\n",
    "            \n",
    "            writer.writeheader()\n",
    "\n",
    "            url_page = f\"https://myanimelist.net/topanime.php?limit={i*50}\"\n",
    "            req_list = requests.get(url_page)\n",
    "            soup_list = BeautifulSoup(req_list.content, 'html.parser')\n",
    "            shows = soup_list.find_all('tr', class_='ranking-list')\n",
    "\n",
    "            for show in shows:\n",
    "                try:\n",
    "                    link = show.find('a').get('href')\n",
    "                    data_row = createRow(link)\n",
    "                    writer.writerow(data_row)\n",
    "                    \n",
    "                    time.sleep(2) # These are required to stop the website from blocking us\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Encountered error '{e}' at '{link}'.\")\n",
    "                    \n",
    "                    time.sleep(2)\n",
    "    \n",
    "    print(\"Dataset creation complete!\")\n",
    "    \n",
    "csv_dir = os.path.join(os.getcwd(), 'csv')\n",
    "genDataset(300, csv_dir, 'mal-data-20-11-2020', start_page=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate CSV files and put them into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "csv_path = os.path.join(os.getcwd(), 'csv')\n",
    "\n",
    "dfs = []\n",
    "for csv in os.listdir(csv_path):\n",
    "    dfs.append(pd.read_csv(os.path.join(data_path, csv), index_col=0))\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type conversion and data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['episodes'] != 'Unknown']\n",
    "df['episodes'] = df['episodes'].astype(int)\n",
    "\n",
    "df['favorites'] = df['favorites'].astype(int)\n",
    "\n",
    "df['members'] = df['members'].astype(int)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseScoreAndScorer(score_and_scorers_raw, position):\n",
    "    try:\n",
    "        numerical_version = float(score_and_scorers_raw.split(', ')[position].strip())\n",
    "        return numerical_version\n",
    "    except:\n",
    "        return pd.NaT\n",
    "    \n",
    "df['score'] = df['score_and_scorers'].apply(lambda s: parseScoreAndScorer(s, 0)).astype(float)\n",
    "df['scorer'] = df['score_and_scorers'].apply(lambda s: parseScoreAndScorer(s, 1)).astype(int)\n",
    "df = df.dropna()\n",
    "df = df.drop('score_and_scorers', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcastParse(broadcast_raw): # This parses through the broadcast time\n",
    "    broadcast_split = [bc_time.strip() for bc_time in broadcast_raw.split(' at ')]\n",
    "    if (len(broadcast_split) == 1) or ('Unknown' in broadcast_split):\n",
    "        return pd.NaT\n",
    "    broadcast_split[0] = broadcast_split[0][:-1]\n",
    "    broadcast_split[1] = broadcast_split[1][:5]\n",
    "    broadcast_string = \" \".join(broadcast_split)\n",
    "    \n",
    "    return broadcast_string\n",
    "\n",
    "df['broadcast_time'] = df['broadcast_time'].apply(broadcastParse)\n",
    "df = df.dropna()\n",
    "df['broadcast_time'] = pd.to_datetime(df['broadcast_time'], format='%A %H %M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one needs to parse the minutes/hours for the episode length\n",
    "def timeParse(time_raw):\n",
    "    time_split = time_raw.split(' ')\n",
    "    if time_split[1] == 'min.':\n",
    "        return int(time_split[0])\n",
    "    elif time_split[1] == 'hr.':\n",
    "        if time_split[3] == 'min.':\n",
    "            return int(time_split[0]) * 60 + int(time_split[2])\n",
    "        else:\n",
    "            return int(time_split[0]) * 60 # This will make sure that if there is only an hour it will not break\n",
    "    else:\n",
    "        return pd.NaT\n",
    "    \n",
    "df['episode_length'] = df['episode_length'].apply(timeParse)\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAired(aired_raw, position):\n",
    "    aired_split = aired_raw.split(' to ')\n",
    "    try:\n",
    "        aired_parsed = \" \".join(aired_split[position].strip().split(\", \"))\n",
    "        aired_formatted = str(time.strftime('%d %m %Y', time.strptime(aired_parsed, '%b %d %Y')))\n",
    "        return aired_formatted\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "df['aired_start'] = pd.to_datetime(df['aired'].apply(lambda s: parseAired(s, 0)), format='%d %m %Y')\n",
    "df['aired_end'] = pd.to_datetime(df['aired'].apply(lambda s: parseAired(s, 1)), format='%d %m %Y')\n",
    "df = df.drop('aired', axis=1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: What is the highest rated show in consideration with the amount of users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To solve this we say the rating is the probability of an anime being good, then we take the Z-scores \n",
    "# and we look at the highest Z-score which means it will be the anime that exceeds the average \n",
    "# chance of it being good based on its members\n",
    "good_probability = df['score'].mean() / 10\n",
    "probabilities = df['score'] / 10\n",
    "stds = (good_probability * (1 - good_probability) / df['members']) ** 0.5\n",
    "weighted_rating = (probabilities - good_probability) / stds\n",
    "\n",
    "weighted_rating.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The best anime with the highest score based on its users is Fullmetal Alchemist as it has the highest Z-score above the mean rating based on it's users compared to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: What is the most popular genre on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do this we are going to have to gather all of the different genre's and then make a new dataframe with these genres\n",
    "# and then look at the one with the highest memberbase\n",
    "df = df[df['producers'] != 'add some']\n",
    "df = df[df['licensors'] != 'add some']\n",
    "df = df[df['genres'] != 'add some']\n",
    "\n",
    "genres = []\n",
    "def makeDataList(array_string, data_list):\n",
    "    split_string = array_string.split(\", \")\n",
    "    for item in split_string:\n",
    "        if item not in data_list:\n",
    "            data_list.append(item)\n",
    "\n",
    "df['genres'].apply(lambda s: makeDataList(s, genres))\n",
    "\n",
    "genre_df = pd.DataFrame()\n",
    "\n",
    "averages = []\n",
    "for genre in genres:\n",
    "    dummy_df = df[df['genres'].str.contains(genre)]\n",
    "    average_members = dummy_df['members'].mean()\n",
    "    averages.append(average_members)\n",
    "\n",
    "genre_df['genres'] = genres\n",
    "genre_df['averages'] = averages\n",
    "\n",
    "genre_df.plot.bar(x='genres', y='averages', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most popular genre on average was police, followed by mecha and then super power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: What is the best month of the to launch a show for the most viewers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make seperate columns of the hour and then group by the hour under the average\n",
    "viewers_df = df.copy()\n",
    "viewers_df['aired_month'] = viewers_df['aired_start'].dt.month # Sort this dataframe by the column\n",
    "\n",
    "viewers_df = viewers_df.groupby('aired_month').mean()\n",
    "viewers_df.index = pd.to_datetime(viewers_df.index, format='%m').month_name().str[:3]\n",
    "viewers_df = viewers_df.drop(['episodes', 'episode_length', 'favorites', 'score', 'scorer'], axis=1)\n",
    "\n",
    "viewers_df.plot.bar(grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most successful anime's with the largest amount of watchers on average were released in April, followed by May, meaning these are the best times to put out an anime for the most amount of watches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: What was the best time to broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make a seperate column for the broadcast time in terms of its hour during the day\n",
    "bc_df = df.copy()\n",
    "bc_df['broadcast_hour'] = bc_df['broadcast_time'].dt.hour\n",
    "bc_df = bc_df.groupby('broadcast_hour').mean()\n",
    "bc_df = bc_df.drop(['episodes', 'episode_length', 'favorites', 'score', 'scorer'], axis=1)\n",
    "\n",
    "bc_df.plot.bar(grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The shows with the highest amount of watchers were released around 10:00AM, therefore the best time to release a show to maximize viewers is at 10:00AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: What studios have the best ratings with consideration for the amount of users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the standard deviation except this time use the data from the other Z-Scores to find where the studios are the best\n",
    "# Using the same parsing function defined in the popular genre\n",
    "\n",
    "studios = []\n",
    "df['studios'].apply(lambda s: makeDataList(s, studios))\n",
    "\n",
    "studio_df = pd.DataFrame()\n",
    "\n",
    "average_ratings = []\n",
    "average_members = []\n",
    "for studio in studios:\n",
    "    dummy_df = df[df['studios'].str.contains(studio)]\n",
    "    rating_average = dummy_df['score'].mean()\n",
    "    member_average = dummy_df['members'].mean()\n",
    "    average_ratings.append(rating_average)\n",
    "    average_members.append(member_average)\n",
    "\n",
    "studio_df['studios'] = studios\n",
    "studio_df['average_ratings'] = average_ratings\n",
    "studio_df['average_members'] = average_members\n",
    "\n",
    "good_probability = studio_df['average_ratings'].mean() / 10\n",
    "probabilities = studio_df['average_ratings'] / 10\n",
    "stds = (good_probability * (1 - good_probability) / studio_df['average_members']) ** 0.5\n",
    "studio_df['weighted_rating'] = (probabilities - good_probability) / stds\n",
    "\n",
    "studio_df.plot.bar(x='studios', y='weighted_rating', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The studio with the highest average score across all of the scorers is Bandai Namco Pictures, and the studio with the least score across all scorers is Imagin studios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: What licensors have the best ratings with consideration for the amount of users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar question to question 5\n",
    "\n",
    "licensors = []\n",
    "df['licensors'].apply(lambda s: makeDataList(s, licensors))\n",
    "\n",
    "licensor_df = pd.DataFrame()\n",
    "\n",
    "average_ratings = []\n",
    "average_members = []\n",
    "for licensor in licensors:\n",
    "    dummy_df = df[df['licensors'].str.contains(licensor)]\n",
    "    rating_average = dummy_df['score'].mean()\n",
    "    member_average = dummy_df['members'].mean()\n",
    "    average_ratings.append(rating_average)\n",
    "    average_members.append(member_average)\n",
    "\n",
    "licensor_df['licensors'] = licensors\n",
    "licensor_df['average_ratings'] = average_ratings\n",
    "licensor_df['average_members'] = average_members\n",
    "\n",
    "good_probability = licensor_df['average_ratings'].mean() / 10\n",
    "probabilities = licensor_df['average_ratings'] / 10\n",
    "stds = (good_probability * (1 - good_probability) / licensor_df['average_members']) ** 0.5\n",
    "licensor_df['weighted_rating'] = (probabilities - good_probability) / stds\n",
    "\n",
    "licensor_df.plot.bar(x='licensors', y='weighted_rating', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The licensor with the most ratings considering the amount of scorers is Crunchyroll, and the licensor with the least is Netflix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocessWord(word):\n",
    "    new_word = word.lower()\n",
    "    word_chars = [char for char in new_word if char in string.ascii_lowercase]\n",
    "    \n",
    "    return \"\".join(word_chars)\n",
    "        \n",
    "def preprocessText(description, studios, licensors, producers, genres):\n",
    "    total = []\n",
    "    \n",
    "    studios_split = studios.split(',')\n",
    "    for studio in studios_split:\n",
    "        total.append(preprocessWord(studio))\n",
    "        \n",
    "    licensors_split = licensors.split(',')\n",
    "    for licensor in licensors_split:\n",
    "        total.append(preprocessWord(licensor))\n",
    "    \n",
    "    producers_split = producers.split(',')\n",
    "    for producer in producers_split:\n",
    "        total.append(preprocessWord(producer))\n",
    "    \n",
    "    genres_split = genres.split(',')\n",
    "    for genre in genres_split:\n",
    "        total.append(preprocessWord(genre))\n",
    "    \n",
    "    description_split = description.split(' ')\n",
    "    for word in description_split:\n",
    "        total.append(preprocessWord(word))\n",
    "        \n",
    "    return \" \".join(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelData(df):\n",
    "    row_num = len(df.index)\n",
    "    text_data = []\n",
    "    score_data = []\n",
    "    \n",
    "    for i in range(row_num):\n",
    "        row = df.iloc[i]\n",
    "        description = row[14]\n",
    "        studios = row[7]\n",
    "        licensors = row[6]\n",
    "        producers = row[5]\n",
    "        genres = row[9]\n",
    "        \n",
    "        row_text = preprocessText(description, studios, licensors, producers, genres)\n",
    "        score = row[15] / 10\n",
    "        \n",
    "        text_data.append(row_text)\n",
    "        score_data.append(score)\n",
    "        \n",
    "    return text_data, score_data\n",
    "\n",
    "text_data, score_data = createModelData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to make a word map and a counter of the word\n",
    "from collections import Counter\n",
    "\n",
    "words = \" \".join([text for text in text_data]).split(' ')\n",
    "counter = Counter(words)\n",
    "most_common = counter.most_common()\n",
    "\n",
    "word_map = {word: i+1 for i, (word, count) in enumerate(most_common)}\n",
    "\n",
    "def sentenceToId(sentence, word_map):\n",
    "    sentence_split = sentence.split(' ') # Here we assume that the sentence has been parsed properly\n",
    "    encoded_sentence = [word_map[word] for word in sentence_split]\n",
    "    \n",
    "    return encoded_sentence\n",
    "\n",
    "encoded_text = [sentenceToId(desc, word_map) for desc in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lens = [len(text) for text in encoded_text]\n",
    "series = pd.Series(encoded_lens)\n",
    "\n",
    "print(series.describe())\n",
    "series.hist()\n",
    "\n",
    "mean_raw = series.mean()\n",
    "std_raw = series.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "encoded_text_cleaned = []\n",
    "score_data_cleaned = []\n",
    "for i, length in enumerate(encoded_lens):\n",
    "    z_score = (length - mean_raw) / std_raw\n",
    "    if (z_score > -3) and (z_score < 3):\n",
    "        encoded_text_cleaned.append(encoded_text[i])\n",
    "        score_data_cleaned.append(score_data[i])\n",
    "        \n",
    "encoded_lens_new = [len(text) for text in encoded_text_cleaned]\n",
    "new_series = pd.Series(encoded_lens_new)\n",
    "\n",
    "print(new_series.describe())\n",
    "new_series.hist()\n",
    "\n",
    "mean_text_length = int(np.round(new_series.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding and truncation of words for embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctSize(vector, vector_size):\n",
    "    vec_len = len(vector)\n",
    "    \n",
    "    if vec_len == vector_size:\n",
    "        return vector\n",
    "    \n",
    "    elif vec_len < vector_size:\n",
    "        ret_vector = vector.copy()\n",
    "        for _ in range(vector_size - vec_len):\n",
    "            ret_vector.append(0)\n",
    "    \n",
    "    elif vec_len > vector_size:\n",
    "        return vector[:vector_size]\n",
    "    \n",
    "padded_text = [correctSize(text, mean_text_length) for text in encoded_text_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into sets for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_len = len(padded_text) - len(padded_text) % 10\n",
    "\n",
    "fixed_padded_text = padded_text[:good_len]\n",
    "fixed_score = score_data[:good_len]\n",
    "\n",
    "train_len = int(0.8 * good_len)\n",
    "val_len = int(0.1 * good_len)\n",
    "test_len = int(0.1 * good_len)\n",
    "\n",
    "train_text = np.array(fixed_padded_text[:train_len])\n",
    "train_labels = np.array(fixed_score[:train_len])\n",
    "train_data = (train_text, train_labels)\n",
    "\n",
    "remaining_text = fixed_padded_text[train_len:]\n",
    "remaining_labels = fixed_score[train_len:]\n",
    "\n",
    "val_text = np.array(remaining_text[:val_len])\n",
    "val_labels = np.array(remaining_labels[:val_len])\n",
    "val_data = (val_text, val_labels)\n",
    "\n",
    "test_text = np.array(remaining_text[test_len:])\n",
    "test_labels = np.array(remaining_labels[test_len:])\n",
    "test_data = (test_text, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation, training, loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "def createModel(word_map, mean_text_length):\n",
    "    model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(input_dim=len(word_map) + 1, output_dim=mean_text_length),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.4)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(1e-4, clipnorm=1), metrics='accuracy') # This means we dont need a dense layer...?\n",
    "    \n",
    "    return model\n",
    "\n",
    "def trainModel(model, train_data, epochs, batch_size, validation_data):\n",
    "    history = model.fit(train_data[0], train_data[1], epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def saveModel(model, save_dir):\n",
    "    model_as_json = model.to_json()\n",
    "    with open(os.path.join(save_dir, 'model.json'), 'w') as json_file:\n",
    "        json_file.write(model_as_json)\n",
    "    model.save_weights(os.path.join(save_dir, 'model.h5'))\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'history.p'), 'wb') as pickle_file:\n",
    "        pickle.dump(history, pickle_file)\n",
    "    \n",
    "    print(f\"Saved model to {save_dir}\")\n",
    "    \n",
    "def loadModel(load_dir):\n",
    "    with open(os.path.join(load_dir, 'model.json'), 'r') as json_file:\n",
    "        loaded_json_model = json_file.read()\n",
    "    model = tf.keras.models.model_from_json(loaded_json_model)\n",
    "    model.load_weights(os.path.join(load_dir, 'model.h5'))\n",
    "    \n",
    "    with open(os.path.join(load_dir, 'history.p'), 'rb') as pickle_file:\n",
    "        history = pickle.load(pickle_file)\n",
    "        \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(1e-4, clipnorm=1), metrics='accuracy')\n",
    "    \n",
    "    return model, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
