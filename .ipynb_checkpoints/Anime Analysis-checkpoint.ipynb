{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Analysis by Ben Osborn and OsbornAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project consists of the scraping and creation of a dataset containing information about all anime's listed on MyAnimeList. This data is analysed, and a model is created to predict the anime's rating based on the pages features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parses through the labels from the soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change name to clean label\n",
    "def parseLabel(element):\n",
    "    string = element.text\n",
    "    \n",
    "    split_colens = string.split(':')\n",
    "    removed_label = split_colens[1:]\n",
    "    \n",
    "    for i, label in enumerate(removed_label):\n",
    "        removed_label[i] = label.replace('\\n', '').strip()\n",
    "    \n",
    "    joined = \" \".join(removed_label)\n",
    "    \n",
    "    return joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parses through the list soup elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseList(element): # Have to check that a tags exist for ever single page and developer\n",
    "    ret_list = [a.text for a in element.find_all('a')]\n",
    "    \n",
    "    return \", \".join(ret_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the field names globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = ['name', 'show_type', 'episodes', 'status', 'aired_start', 'aired_end', \n",
    "                   'broadcast_time', 'producers', 'licensors', 'studios', 'source',\n",
    "                   'genres', 'episode_length', 'rating', 'score_and_scorers', 'members',\n",
    "                   'favorites', 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapes the page from the show and returns a row of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRow(url):\n",
    "    global field_names\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "    side_panel = soup.find('td', class_='borderClass')\n",
    "    side_panel_subdiv = side_panel.find('div')\n",
    "    side_panel_divs = side_panel_subdiv.find_all('div')\n",
    "    \n",
    "    ret_dict = {field_name: 'NaT' for field_name in field_names}\n",
    "    \n",
    "    ret_dict['description'] = soup.find('p', itemprop='description').text # --------- Check this one\n",
    "    \n",
    "    for panel in side_panel_divs:\n",
    "        split = str(panel.text.split(':')[0].strip())\n",
    "        if split == \"English\":\n",
    "            print(parseLabel(panel))\n",
    "            ret_dict['name'] = parseLabel(panel)\n",
    "        if split == \"Type\":\n",
    "            ret_dict['show_type'] = parseLabel(panel)\n",
    "        if split == \"Episodes\":\n",
    "            ret_dict['episodes'] = parseLabel(panel)\n",
    "        if split == \"Status\":\n",
    "            ret_dict['status'] = parseLabel(panel)\n",
    "        if split == \"Aired\":\n",
    "            aired_raw = parseLabel(panel)\n",
    "#             Its in here because it goes to '?'\n",
    "            aired = [time.strftime('%d-%m-%Y', time.strptime(date.strip().replace(',', ''), '%b %d %Y')) for date in aired_raw.split(' to ')]\n",
    "            ret_dict['aired_start'] = aired[0]\n",
    "            if len(aired) == 2:\n",
    "                ret_dict['aired_end'] = aired[1]\n",
    "        if split == \"Broadcast\":\n",
    "            broadcast_time_raw = parseLabel(panel)\n",
    "            broadcast_time_split = [element.strip() for element in broadcast_time_raw.split(' at ')]\n",
    "            if len(broadcast_time_split) == 2:\n",
    "                broadcast_time_split[0] = broadcast_time_split[0][:-1]\n",
    "                broadcast_time_split[1] = broadcast_time_split[1][:5]\n",
    "                broadcast_time_joined = \" \".join(broadcast_time_split)\n",
    "                ret_dict['broadcast_time'] = time.strftime('%A %H:%M', time.strptime(broadcast_time_joined, '%A %H %M')) # Good (Possibly unnecessary)\n",
    "        if split == \"Producers\":\n",
    "            ret_dict['producers'] = parseList(panel)\n",
    "        if split == \"Licensors\":\n",
    "            ret_dict['licensors'] = parseList(panel)\n",
    "        if split == \"Studios\":\n",
    "            ret_dict['studios'] = parseList(panel)\n",
    "        if split == \"Source\":\n",
    "            ret_dict['source'] = parseLabel(panel)\n",
    "        if split == \"Genres\":\n",
    "            ret_dict['genres'] = parseList(panel)\n",
    "        if split == \"Duration\":\n",
    "            ret_dict['episode_length'] = parseLabel(panel).split(' ')[0]\n",
    "        if split == \"Rating\":\n",
    "            ret_dict['rating'] = parseLabel(panel).split(' ')[0]\n",
    "        if split == \"Score\":\n",
    "            ret_dict['score_and_scorers'] = \", \".join([part.text for part in panel.find_all('span')][1:])\n",
    "        if split == \"Members\":\n",
    "            ret_dict['members'] = \"\".join(parseLabel(panel).split(','))\n",
    "        if split == \"Favorites\":\n",
    "            ret_dict['favorites'] = \"\".join(parseLabel(panel).split(','))\n",
    "            \n",
    "    return ret_dict\n",
    "    \n",
    "# createRow(\"https://myanimelist.net/anime/28977/Gintama%C2%B0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through the amount of pages specified then scrape the information for each show, then store them to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithread this to make it faster\n",
    "\n",
    "def genDataset(pages_to_scrape, csv_filename, start_page=0): # Where resume is the page of which it left off from\n",
    "    global field_names\n",
    "    \n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "        \n",
    "        for i in range(start_page, pages_to_scrape):\n",
    "            url_page = f\"https://myanimelist.net/topanime.php?limit={i*50}\"\n",
    "            req_list = requests.get(url_page)\n",
    "            soup_list = BeautifulSoup(req_list.content, 'html.parser')\n",
    "            shows = soup_list.find_all('tr', class_='ranking-list')\n",
    "\n",
    "            for j, show in enumerate(shows):\n",
    "                link = show.find('a').get('href')\n",
    "#                 This needs a try block so if it fails for one it will continue to work\n",
    "#                 try:\n",
    "                print(f\"Page {i} show {j}\")\n",
    "                data_row = createRow(link)\n",
    "                writer.writerow(data_row)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Encountered error '{e}' on page {i}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 show 0\n",
      "Fullmetal Alchemist Brotherhood\n",
      "Page 0 show 1\n",
      "Steins;Gate\n",
      "Page 0 show 2\n",
      "Gintama Season 4\n",
      "Page 0 show 3\n",
      "Hunter x Hunter\n",
      "Page 0 show 4\n",
      "Legend of the Galactic Heroes\n",
      "Page 0 show 5\n",
      "Gintama Season 2\n",
      "Page 0 show 6\n",
      "Attack on Titan Season 3 Part 2\n",
      "Page 0 show 7\n",
      "Gintama Enchousen\n",
      "Page 0 show 8\n",
      "March Comes In Like A Lion 2nd Season\n",
      "Page 0 show 9\n",
      "A Silent Voice\n",
      "Page 0 show 10\n",
      "Your Name.\n",
      "Page 0 show 11\n",
      "Gintama Season 5\n",
      "Page 0 show 12\n",
      "Gintama\n",
      "Page 0 show 13\n",
      "Page 0 show 14\n",
      "Clannad ~After Story~\n",
      "Page 0 show 15\n",
      "Owarimonogatari Second Season\n",
      "Page 0 show 16\n",
      "Code Geass Lelouch of the Rebellion R2\n",
      "Page 0 show 17\n",
      "Haikyu!! 3rd Season\n",
      "Page 0 show 18\n",
      "Mob Psycho 100 II\n",
      "Page 0 show 19\n",
      "Gintama. Silver Soul Arc - Second Half War\n",
      "Page 0 show 20\n",
      "Spirited Away\n",
      "Page 0 show 21\n",
      "Page 0 show 22\n",
      "Gintama. Silver Soul Arc\n",
      "Page 0 show 23\n",
      "Cowboy Bebop\n",
      "Page 0 show 24\n",
      "Kaguya-sama Love is War Season 2\n",
      "Page 0 show 25\n",
      "Descending Stories Showa Genroku Rakugo Shinju\n",
      "Page 0 show 26\n",
      "Page 0 show 27\n",
      "Monogatari Series Second Season\n",
      "Page 0 show 28\n",
      "Haikyu!! 2nd Season\n",
      "Page 0 show 29\n",
      "Page 0 show 30\n",
      "Your Lie in April\n",
      "Page 0 show 31\n",
      "Fate/stay night Heaven's Feel - III. Spring Song\n",
      "Page 0 show 32\n",
      "Fighting Spirit\n",
      "Page 0 show 33\n",
      "Made in Abyss\n",
      "Page 0 show 34\n",
      "Monster\n",
      "Page 0 show 35\n",
      "Princess Mononoke\n",
      "Page 0 show 36\n",
      "MUSHI-SHI -Next Passage-\n",
      "Page 0 show 37\n",
      "Samurai X Trust and Betrayal\n",
      "Page 0 show 38\n",
      "Page 0 show 39\n",
      "Page 0 show 40\n",
      "Code Geass Lelouch of the Rebellion\n",
      "Page 0 show 41\n",
      "Great Teacher Onizuka\n",
      "Page 0 show 42\n",
      "Mushi-Shi\n",
      "Page 0 show 43\n",
      "Rascal Does Not Dream of a Dreaming Girl\n",
      "Page 0 show 44\n",
      "Fighting Spirit New Challenger\n",
      "Page 0 show 45\n",
      "Natsume's Book of Friends Season 4\n",
      "Page 0 show 46\n",
      "Rocky Joe 2\n",
      "Page 0 show 47\n",
      "Howl's Moving Castle\n",
      "Page 0 show 48\n",
      "Demon Slayer Kimetsu no Yaiba\n",
      "Page 0 show 49\n",
      "The Disappearance of Haruhi Suzumiya\n",
      "Page 1 show 0\n",
      "Gurren Lagann\n",
      "Page 1 show 1\n",
      "Wolf Children\n",
      "Page 1 show 2\n",
      "Natsume's Book of Friends Season 6\n",
      "Page 1 show 3\n",
      "The Promised Neverland\n",
      "Page 1 show 4\n",
      "Page 1 show 5\n",
      "JoJo's Bizarre Adventure Golden Wind\n",
      "Page 1 show 6\n",
      "Ping Pong the Animation\n",
      "Page 1 show 7\n",
      "Violet Evergarden\n",
      "Page 1 show 8\n",
      "Death Note\n",
      "Page 1 show 9\n",
      "Kizumonogatari Part 2 Nekketsu\n",
      "Page 1 show 10\n",
      "The Tatami Galaxy\n",
      "Page 1 show 11\n",
      "One Punch Man\n",
      "Page 1 show 12\n",
      "Showa Genroku Rakugo Shinju\n",
      "Page 1 show 13\n",
      "Bakuman.\n",
      "Page 1 show 14\n",
      "Fate/stay night Heaven's Feel - II. Lost Butterfly\n",
      "Page 1 show 15\n",
      "Fate/Zero Season 2\n",
      "Page 1 show 16\n",
      "Hajime No Ippo The Fighting!\n",
      "Page 1 show 17\n",
      "I want to eat your pancreas\n",
      "Page 1 show 18\n",
      "Natsume's Book of Friends Season 3\n",
      "Page 1 show 19\n",
      "Natsume's Book of Friends Season 5\n",
      "Page 1 show 20\n",
      "Re ZERO -Starting Life in Another World- Season 2\n",
      "Page 1 show 21\n",
      "A Place Further Than The Universe\n",
      "Page 1 show 22\n",
      "The Shadow that Devours the Sun\n",
      "Page 1 show 23\n",
      "Haikyu!!\n",
      "Page 1 show 24\n",
      "the Garden of sinners Chapter 5 Paradox Paradigm\n",
      "Page 1 show 25\n",
      "Gurren Lagann The Movie The Lights in the Sky are Stars\n",
      "Page 1 show 26\n",
      "Natsume's Book of Friends Season 2\n",
      "Page 1 show 27\n",
      "Assassination Classroom Second Season\n",
      "Page 1 show 28\n",
      "Page 1 show 29\n",
      "KONOSUBA -God's blessing on this wonderful world!- Legend of Crimson\n",
      "Page 1 show 30\n",
      "Aria the Origination\n",
      "Page 1 show 31\n",
      "Page 1 show 32\n",
      "Ghost in the Shell Stand Alone Complex 2nd GIG\n",
      "Page 1 show 33\n",
      "Attack on Titan Season 3\n",
      "Page 1 show 34\n",
      "Slam Dunk\n",
      "Page 1 show 35\n",
      "Space Brothers\n",
      "Page 1 show 36\n",
      "Gintama. Slip Arc\n",
      "Page 1 show 37\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data '?' does not match format '%b %d %Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-378079de49b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mal-data-12-11-2020.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-174-5df4abae5901>\u001b[0m in \u001b[0;36mgenDataset\u001b[0;34m(pages_to_scrape, csv_filename, start_page)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#                 try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Page {i} show {j}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mdata_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#                 except Exception as e:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-e4486e165289>\u001b[0m in \u001b[0;36mcreateRow\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Aired\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0maired_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpanel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d-%m-%Y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%b %d %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maired_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' to '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mret_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aired_start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maired\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maired\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-176-e4486e165289>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Aired\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0maired_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpanel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d-%m-%Y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%b %d %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maired_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' to '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mret_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aired_start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maired\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maired\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime_time\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \"\"\"Return a time struct based on the input string and the\n\u001b[1;32m    561\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STRUCT_TM_ITEMS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[1;32m    350\u001b[0m                          (data_string, format))\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data '?' does not match format '%b %d %Y'"
     ]
    }
   ],
   "source": [
    "genDataset(10, 'mal-data-12-11-2020.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
